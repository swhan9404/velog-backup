---
title: "ì‹ ê²½ë§ Neural network"
description: "ë¹„ì„ í˜• ëª¨ë¸ì¸ ì‹ ê²½ë§(Neural network) ì˜ ê¸°ë³¸ì ì¸ ê°œë…ê³¼ ê·¸ ì˜ë¯¸ë¥¼ í’€ì–´ë‚˜ê°€ê¸° ìœ„í•œ ë„êµ¬ë“¤ì„ ì •ë¦¬í•´ë³¸ë‹¤.ì‹ ê²½ë§ì˜ ê°œë…ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë  ë¬¸ì œë“¤ì„ ì‚´í´ë³´ë„ë¡ í•œë‹¤.ì„ í˜•íšŒê·€(ë‹¤ì¤‘ì„ í˜•íšŒê·€,Multiple Linear Regression) ëŠ” ìˆ˜ì¹˜í˜• ì„¤ëª…ë³€ìˆ˜"
date: 2021-03-07T08:58:52.650Z
tags: ["ì¸ê³µì§€ëŠ¥ìˆ˜í•™"]
---
# ğŸ“Œ Neural network

**ë¹„ì„ í˜• ëª¨ë¸ì¸ ì‹ ê²½ë§(Neural network)** ì˜ ê¸°ë³¸ì ì¸ ê°œë…ê³¼ ê·¸ ì˜ë¯¸ë¥¼ í’€ì–´ë‚˜ê°€ê¸° ìœ„í•œ ë„êµ¬ë“¤ì„ ì •ë¦¬í•´ë³¸ë‹¤.

------

# ğŸ“„ ì²˜ìŒìœ¼ë¡œ ë˜ëŒì•„ê°€ë³¼ê¹Œ?

ì‹ ê²½ë§ì˜ ê°œë…ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë  ë¬¸ì œë“¤ì„ ì‚´í´ë³´ë„ë¡ í•œë‹¤.

## âœï¸ ì„ í˜•íšŒê·€(Linear Regression)

![](../images/7a1845fd-3170-476c-942a-fd03dbd64d47-image.png)

**ì„ í˜•íšŒê·€(ë‹¤ì¤‘ì„ í˜•íšŒê·€,Multiple Linear Regression)** ëŠ” ìˆ˜ì¹˜í˜• ì„¤ëª…ë³€ìˆ˜ **X** ì™€ ì—°ì†í˜• ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ì¢…ì†ë³€ìˆ˜ **Y** ê°„ì˜ ê´€ê³„ë¥¼ ì„ í˜•ìœ¼ë¡œ ê°€ì •í•˜ê³  ì´ë¥¼ ì˜ í‘œí˜„í• ìˆ˜ ìˆëŠ” **íšŒê·€ê³„ìˆ˜ \*Î²\* ë¥¼ ë°ì´í„°ë¡œë¶€í„° ì¶”ì •í•˜ëŠ” ëª¨ë¸** ì´ë‹¤.

$$
y = f(x) = \beta_{0} + \beta_{1}x_{1} + ... + \beta_{p}x_{p} = \beta^{T}\overrightarrow{x},\\ \overrightarrow{x} ^{T} = [1, x_{1}, ... , x_{p}] \\ \beta^{T} = [\beta_{0},\beta_{1}, ... , \beta_{p}]
$$
ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í• ìˆ˜ ìˆëŠ” íšŒê·€ê³„ìˆ˜ *Î²* ë¥¼ ì°¾ê¸° ìœ„í•´ì„œ **ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’(\**y\**^)ê³¼ ì‹¤ì œê°’(\**y\**)ì˜ ì°¨ì´, ì¦‰ ìµœì†Œì œê³±ë²•(Least Squared Method)ë¥¼ ì´ìš©í•˜ì—¬ ì”ì°¨(Residual)ì˜ í•©ì´ ìµœì†Œê°€ ë˜ë„ë¡ í•œë‹¤.**

$$
\sum residual^{2} = \sum_{i}^{}(y_{i}-f(x_{i}, \beta))^{2}
$$


## âœï¸ ë‹¤í•­ íšŒê·€(Polynomial Regression)

![](../images/ac2bd052-8bd5-45b9-9a1d-2c4e8dd5b45f-image.png)

ì„ í˜•íšŒê·€ë¥¼ í™•ì¥í•˜ì—¬ ë‹¨ìˆœ ì§ì„ ì´ ì•„ë‹Œ **dì°¨ ë‹¤í•­í•¨ìˆ˜** ì— ëŒ€í•´ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤.

$$
y= \beta_{0} + \beta_{1}x
$$
ë¥¼ í™•ì¥í•˜ì—¬

$$
y= \beta_{0} + \beta_{1}x^{1}+ \beta_{2}x^{2}+ ... + \beta_{d}x^{d}
$$
ë‹¤í•­ íšŒê·€ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

## âœï¸ ë¡œì§€ìŠ¤í‹±íšŒê·€(Logistic Regression)

![](../images/6e30bcbb-d4ed-4b06-af1b-32d672bf731c-image.png)

ì—°ì†í˜• ìˆ«ìê°€ ì•„ë‹Œ **ë²”ì£¼í˜•(Categorical) ì¢…ì† ë³€ìˆ˜** ëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë²”ì£¼í˜• ë°ì´í„°ì˜ ìˆ«ì ìì²´ê°€ ì˜ë¯¸ë¥¼ ì§€ë‹ˆê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ë‹¤ì¤‘ì„ í˜•íšŒê·€(Multiple Linear Regression)ë¥¼ ì ìš©í• ìˆ˜ ì—†ë‹¤. ê·¸ë˜ì„œ ì œì•ˆëœ ê²ƒì´ **ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸** ì´ë©°, ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ í•˜ë‚˜ì”© ì‚´í´ë³´ë„ë¡ í•œë‹¤.

- **Odds**
  $$
  \frac{ì‚¬ê±´Aê°€ ë°œìƒí•  í™•ë¥ }{ì‚¬ê±´Aê°€ ë°œìƒí•˜ì§€ ì•Šì„ í™•ë¥ } = \frac{P(A)}{P(A^{c})} = \frac{P(A)}{1-P(A)}
  $$
  odds ëŠ” *P*(*A*)ì˜ ê°’ì´ ì»¤ì§„ë‹¤ë©´(1ì— ê°€ê¹Œì›Œì§„ë‹¤ë©´) ì¦ê°€ / ê°’ì´ ì‘ì•„ì§„ë‹¤ë©´(0ì— ê°€ê¹Œì›Œì§„ë‹¤ë©´) ê°ì†Œí•˜ê²Œ ëœë‹¤. **ë‹¤ë¥´ê²Œ ìƒê°í•œë‹¤ë©´ oddsê°’ì´ í¬ë‹¤ëŠ” ê²ƒì€ ì‚¬ê±´ Aê°€ ë°œìƒí•  í™•ë¥ ì´ ì»¤ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.**

![](../images/1b83961a-4206-4d1a-aa71-b91d37a5a3e8-image.png)

- ì´í•­ ë¡œì§€ìŠ¤í‹± íšŒê·€(Binomial Logistic Regression)

  í´ë˜ìŠ¤ê°€ 2ê°œì¸ ì´ì§„ë¶„ë¥˜ ë¬¸ì œ

   

  ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´

   

  ê¸°ì¡´ì˜ íšŒê·€ëª¨ë¸ê³¼ oddsë¥¼ ê²°í•©í•œ ì´í•­ ë¡œì§€ìŠ¤í‹±íšŒê·€ ëª¨ë¸

   

  ì„ ìƒì„±í•œë‹¤.

  $$
log(\frac{P(Y_{1}|x)}{1-P(Y_{1}|x)}) = \beta^{T}x
  $$
  
  
  ```null
  log ë¥¼ ì·¨í•œê²ƒì€ odds ê°’ì˜ ë²”ìœ„ë¥¼ ìš°ë³€ì˜ ê°’ ë²”ìœ„ì™€ ë§ì¶°ì£¼ê¸° ìœ„í•¨ì´ë‹¤.
  ```

$$
\frac{P(Y_{1}|x)}{1-P(Y_{1}|x)} = e^{\beta^{T}x} \\ P(Y_{1}|x) = e^{\beta^{T}x}(1-P(Y_{1}|x)) = e^{\beta^{T}x}- e^{\beta^{T}x}P(Y_{1}|x) \\ P(Y_{1}|x)(1+e^{\beta^{T}x}) = e^{\beta^{T}x} \\ \\ \therefore P(Y_{1}|x) = \frac{e^{\beta^{T}x}}{1+e^{\beta^{T}x}}= \frac{1}{e^{-\beta^{T}x}}
$$

![](../images/899fb382-04e6-4e39-a71e-886521d114a9-image.png)

- **ê²°ì •ê²½ê³„(Decision Boundary)**
  ì„ì˜ì˜ ì…ë ¥ ë²¡í„° **x** ì˜ í´ë˜ìŠ¤ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ì€ *P*(*Y*=1âˆ£*X*=**x**) ì™€ *P*(*Y*=0âˆ£*X*=**x**) ì˜ ê°’ì„ ë¹„êµí•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒ ìˆ˜ì‹ì„ í†µí•´ **ê²°ì • ê²½ê³„ëŠ”\*Î²^{T} x**=0 ì¸ hyperplane** ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

$$
P(Y_{1}|x) > P(Y_{0}|x) \\ \frac{P(Y_{1}|x)}{P(Y_{0}|x)} > 1 \\ log(\frac{P(Y_{1}|x)}{1-P(Y_{1}|x)}) >0 \\ \therefore \beta^{T}x >0
$$

![](../images/6de3a9d8-00e9-4500-ab28-ef4d9081c1a9-image.png)

- ë‹¤í•­ ë¡œì§€ìŠ¤í‹± íšŒê·€(Multinomial Logistic Regression)

  ë§Œì•½

   

  í´ë˜ìŠ¤ê°€ 3ê°œ ì´ìƒ

   

  ì´ë¼ë©´ ë‹¤ìŒê³¼ ê°™ì´

   

  2ê°œì˜ ì´í•­ ë¡œì§€ìŠ¤í‹± íšŒê·€

   

  ë¡œ ë¬¸ì œë¥¼ í•´ê²°í• ìˆ˜ ìˆë‹¤.

  (1) 
$$
  log(\frac{Y_{1}|x}{P(Y_{3}|x)}) = \beta_{1}^{T}x \\ P(Y_{1}|x) = e^{\beta_{1}^{T}x } ...(*1) \\
$$
  (2) 
$$
  log(\frac{Y_{2}|x}{P(Y_{3}|x)}) = \beta_{2}^{T}x \\ P(Y_{2}|x) = e^{\beta_{2}^{T}x }P(Y_{3}|x) ...(*2) \\
$$


(3) 
$$
P(Y_{3}|x) = 1-P(Y_{1}|x) - P(Y_{2}|x) \\ P(Y_{3}|x) = 1 - (*1) - (*2) \\ P(Y_{3}|x)(1+e^{\beta_{1}x}+e^{\beta_{2}x})) =1 \\ \\ \therefore P(Y_{3}|x) = \frac{1}{1+e^{\beta_{1}x}+e^{\beta_{2}x})}
$$
í™•ì¥í•˜ì—¬ *K*ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
P(Y_{k}|X=x) = \frac{e^{\beta_{k}^{T}x}}{1+\sum_{i=1}^{K-1}e^{\beta_{i}^{T}x}}, (k=0,1, ..., K-1) \\ P(Y_{K}|X=x) = \frac{1}{1+\sum_{i=1}^{K-1}e^{\beta_{i}^{T}x}}
$$
ì˜ ë‘ ì‹ì„ ì¼ë°˜í™”í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
P(Y_{i}|x) = e^{ \beta _{i}^{T}x}P(Y_{K}|x)\\ = e^{ \beta _{i}^{T}x} \frac{1}{1+\sum_{i=1}^{K-1} e^{ \beta _{i}^{T}x} } \\ = \frac{e^{ \beta _{K}^{T}x} }{e^{ \beta _{i}^{T}x} + \sum_{i=1}^{K-1} e^{ \beta _{i}^{T}x} }  (\because P(Y_{K}|x) = e^{ \beta _{K}^{T}x} P( Y_{K}|x)) \\ \therefore P(Y_{i}|x) = \frac{e^{ \beta _{i}^{T}x}}{\sum_{K}^{i=1}e^{ \beta _{i}^{T}x}}
$$


## âœï¸ ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ (Single-layer Perceptron)

í¼ì…‰íŠ¸ë¡ ì€ **ë‹¤ìˆ˜ì˜ ì‹ í˜¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ step function ì„ ì´ìš©í•˜ì—¬ ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ 0 ë˜ëŠ” 1ì˜ ì‹ í˜¸ë¥¼ ì¶œë ¥í•œë‹¤.** ë‹¤ìŒì€ ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì„ í‘œí˜„í•˜ì˜€ë‹¤.

![](../images/5bdc5bc7-aed8-4af6-b87b-ad5d4518b3eb-image.png)

![](../images/e5405aa4-e5cf-4872-820b-1d67ca68bcbe-image.png)
$$
\left\{\begin{matrix} y = 1, if \sum_{i=1}^{n} x_{i}w_{i} = w^{T}x >= \theta \\ y = 0, if \sum_{i=1}^{n} x_{i}w_{i} = w^{T}x < \theta \end{matrix}\right.
$$
ìœ„ì˜ ì‹ì—ì„œ ì„ê³„ê°’ì„ ì¢Œë³€ìœ¼ë¡œ ë„˜ê¸°ê³  **í¸í–¥(b,bias)** ìœ¼ë¡œ í‘œí˜„í• ìˆ˜ë„ ìˆëŠ”ë°, ë³´í†µ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. (ë‹¤ìŒ ê·¸ë¦¼ì—ì„œëŠ” *w**o* ê°€ bias ë¡œ í‘œí˜„ë˜ì—ˆë‹¤.)

![](../images/cd1b74a1-07a0-43e3-a608-099b3938a940-image.png)

$$
\left\{\begin{matrix} y = 1, if \sum_{i=1}^{n} x_{i}w_{i} +w_{0}= w^{T}x +w_{0}>= \theta \\ y = 0, if \sum_{i=1}^{n} x_{i}w_{i}+w_{0} = w^{T}x +w_{0}< \theta \end{matrix}\right.
$$
ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì„ ì´ìš©í•˜ë©´ **AND,NAND,OR ê²Œì´íŠ¸** ë¥¼ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
![](../images/9f62ae35-691d-42d5-9c35-1bec936c44cf-image.png)

- AND ê²Œì´íŠ¸

![](../images/2f3918ec-6f45-40d1-8113-c703696cd615-image.png)

- **NAND ê²Œì´íŠ¸**

![](../images/00de7860-9a93-407c-8b63-91dce18998a6-image.png)

- **OR ê²Œì´íŠ¸**

![](../images/7e0abeab-c918-4f7f-a230-2f0c93888580-image.png)

## âœï¸ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-layer Perceptron)

ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ AND,NAND,OR ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆì—ˆì§€ë§Œ XOR ë¬¸ì œëŠ” í’€ ìˆ˜ ì—†ì—ˆë‹¤. **ì¦‰ XOR ë¬¸ì œëŠ” ì§ì„  í•˜ë‚˜ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ(ì„ í˜•ë¶„ë¦¬)ì€ ë¶ˆê°€ëŠ¥í•˜ì˜€ê³ , ë¹„ì„ í˜• ì˜ì—­ìœ¼ë¡œ ë¶„ë¦¬í–ˆì–´ì•¼ í–ˆë‹¤.**

![](../images/0be204ae-b3af-4503-b3d4-b37901d5a9cc-image.png)

![](../images/18f62235-ca90-4d52-9708-c344e3ed6a10-image.png)

![](../images/bc1e6fc2-9a04-4073-8ad6-109721a168e3-image.png)

ê·¸ë˜ì„œ **ì€ë‹‰ì¸µ(Hidden layer)ì„ ì¶”ê°€í•˜ì—¬ XOR ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ ì œì•ˆëœ ê²ƒì´ ë°”ë¡œ MLP(Multi-layer Perceptron) ì´ë‹¤.**

![](../images/58059543-29be-40d3-ad1f-1c11b40d540b-image.png)

![](../images/fb3bccd1-18da-4375-b6ff-573f75dd6bbd-image.png)

ì´í›„ë¡œ ë” ë³µì¡í•œ ë¬¸ì œì„ í’€ê¸° ìœ„í•´ ì€ë‹‰ì¸µì€ ëŠ˜ë ¤ì¡Œê³ , ì•„ë˜ì™€ ê°™ì´ **ì€í‹±ì¸µì´ 2ê°œ ì´ìƒì¸ ì‹ ê²½ë§ì„ ì‹¬ì¸µ ì‹ ê²½ë§(Deep Neural Network, DNN)** ì´ë¼ê³  í•œë‹¤.

![](../images/5ef6fed9-515a-4b0a-838e-05055cf363b9-image.png)

------

# ğŸ“„ ì´ì œ ì‹ ê²½ë§ì„ ì‚´í´ë³¼ê¹Œ?

ì‹ ê²½ë§ì„ ìˆ˜ì‹ìœ¼ë¡œ ì‚´í´ë³¸ë‹¤ë©´ **ê° ë°ì´í„°ì— ëŒ€í•´ dê°œì˜ ë³€ìˆ˜ë¡œ pê°œì˜ ì„ í˜•ëª¨ë¸** ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤.

![](../images/b0236603-3a9a-4020-8d0a-cca38fc74c6b-image.png)

![](../images/1587e943-13bc-4467-ab6f-d290acd3509b-image.png)

ì‹ ê²½ë§ì—ì„œ ì¶œë ¥ë²¡í„° **O** ë¥¼ ì–»ê¸° ì „ [í™œì„±í•¨ìˆ˜](https://velog.io/@kgh732/ë¶€ìŠ¤íŠ¸ìº í”„-AI-Tech-U-stage.-2-3#-í™œì„±í•¨ìˆ˜activation-functionëŠ”-ë¬´ì—‡ì¼ê¹Œ) *Ïƒ* ê°€ ì ìš©ë˜ëŠ” í•˜ë‚˜ì˜ ì€ë‹‰ì¸µ **H** ë¥¼ ìƒê°í•´ë³¸ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ê° ì ì¬ë²¡í„° **z***i*=(*z*1,*z*2,â€¦,*z**p*)ì— í™œì„±í•¨ìˆ˜ *Ïƒ* ë¥¼ ì ìš©í•˜ì—¬ ìƒì„±ëœ **H**=(*Ïƒ*(**z**1),*Ïƒ*(**z**2),â€¦,*Ïƒ*(**z***n*)) ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

![](../images/50994112-e6b9-4d00-b8c3-e132a7927bc9-image.png)

ì¶œë ¥ë²¡í„° **O** ë¥¼ ì–»ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ í–‰ë ¬ **W**(2) ê³¼ **b**(2) ë¥¼ í†µí•´ ì„ í˜•ë³€í™˜í•˜ë©´ (**W**(2),**W**(1)) ì„ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§€ëŠ” **2-layers Neural Net(1-hidden-layer Neural Net)** ì´ ë§Œë“¤ì–´ì§„ë‹¤.

![](../images/01cefc2e-3ced-49ab-85a7-d679fdb08d71-image.png)

![](../images/4a34920d-da2f-434c-9e35-1dbaa3bc017b-image.png)

------

# ğŸ“„ í™œì„±í•¨ìˆ˜(Activation Function)ëŠ” ë¬´ì—‡ì¼ê¹Œ?

- **í™œì„±í•¨ìˆ˜ëŠ” ì…ë ¥ ì‹ í˜¸ì˜ ì´í•© ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¶œë ¥ ì‹ í˜¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜** ì…ë‹ˆë‹¤.  
- **í™œì„±í•¨ìˆ˜ëŠ” ì…ë ¥ ì‹ í˜¸ì˜ ì´í•©ì´ í™œì„±í™” ë˜ëŠ”ì§€ ì•„ë‹Œì§€ë¥¼ ì •í•˜ëŠ” ì—­í• **ì„ í•©ë‹ˆë‹¤.  

### âœï¸ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid)

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” Logistic í•¨ìˆ˜ë¼ ë¶ˆë¦¬ê¸°ë„í•œë‹¤. 

- ì„ í˜•ì¸ ë©€í‹°í¼ì…‰íŠ¸ë¡ ì—ì„œ ë¹„ì„ í˜• ê°’ì„ ì–»ê¸° ìœ„í•´ ì‚¬ìš©í•˜ê¸° ì‹œì‘í–ˆë‹¤. 

- ì‹œê·¸ëª¨ì´ë“œì˜ íŠ¹ì§•

  - ìš°ì„  í•¨ìˆ˜ê°’ì´ (0, 1)ë¡œ ì œí•œëœë‹¤.
  - ì¤‘ê°„ ê°’ì€ 1/2ì´ë‹¤.
  - ë§¤ìš° í° ê°’ì„ ê°€ì§€ë©´ í•¨ìˆ˜ê°’ì€ ê±°ì˜ 1ì´ë©°, ë§¤ìš° ì‘ì€ ê°’ì„ ê°€ì§€ë©´ ê±°ì˜ 0ì´ë‹¤.

- ì‹œê·¸ëª¨ì´ë“œì˜ ë‹¨ì  - ì‹ ê²½ë§ ì´ˆê¸°ì—ëŠ” ë§ì´ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ, ìµœê·¼ì—ëŠ” ì•„ë˜ì˜ ë‹¨ì ë“¤ ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

  - **Gradient Vanishing** í˜„ìƒì´ ë°œìƒí•œë‹¤. ë¯¸ë¶„í•¨ìˆ˜ì— ëŒ€í•´ x=0x=0ì—ì„œ ìµœëŒ€ê°’ 1/4ì„ ê°€ì§€ê³ , inputê°’ì´ ì¼ì •ì´ìƒ ì˜¬ë¼ê°€ë©´ ë¯¸ë¶„ê°’ì´ ê±°ì˜ 0ì— ìˆ˜ë ´í•˜ê²Œëœë‹¤. ì´ëŠ” |x|ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ Gradient Backpropagationì‹œ ë¯¸ë¶„ê°’ì´ ì†Œì‹¤ë  ê°€ëŠ¥ì„±ì´ í¬ë‹¤.

  - **í•¨ìˆ˜ê°’ ì¤‘ì‹¬ì´ 0ì´ ì•„ë‹ˆë‹¤.** í•¨ìˆ˜ê°’ ì¤‘ì‹¬ì´ 0ì´ ì•„ë‹ˆë¼ í•™ìŠµì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆë‹¤. ê·¸ ì´ìœ ë¥¼ ì•Œì•„ë³´ë©´. ë§Œì•½ ëª¨ë“  xê°’ë“¤ì´ ê°™ì€ ë¶€í˜¸(ex. for all x is positive) ë¼ê³  ê°€ì •í•œë‹¤. (ë°‘ì˜ ìˆ˜ì‹ ì°¸ê³ ). ë”°ë¼ì„œ í•œ ë…¸ë“œì— ëŒ€í•´ ëª¨ë“  íŒŒë¼ë¯¸í„°wì˜ ë¯¸ë¶„ê°’ì€ ëª¨ë‘ ê°™ì€ ë¶€í˜¸ë¥¼ ê°™ê²Œëœë‹¤. ë”°ë¼ì„œ ê°™ì€ ë°©í–¥ìœ¼ë¡œ updateë˜ëŠ”ë° ì´ëŸ¬í•œ ê³¼ì •ì€ í•™ìŠµì„ zigzag í˜•íƒœë¡œ ë§Œë“¤ì–´ ëŠë¦¬ê²Œ ë§Œë“œëŠ” ì›ì¸ì´ ëœë‹¤.
    $$
    \frac{\partial L}{ \partial w} = \frac{\partial L}{ \partial a} \frac{\partial a}{\partial w} \\ \frac{ \partial L}{\partial w} = \frac {\partial L}{ \partial a}x (\because \frac{ \partial a}{\partial w} = x ) \\ \frac{ \partial L}{\partial w} ëŠ” \frac {\partial L}{ \partial a} ë¶€í˜¸ì— ì˜í•´ ê²°ì • (\because \forall x >0 )
    $$
    
- exp í•¨ìˆ˜ ì‚¬ìš©ì‹œ ë¹„ìš©ì´ í¬ë‹¤.

$$
\sigma (x) = \frac{1}{1+e^{-x}} \\ \sigma^{'} = \sigma(x)(1-\sigma(x))
$$

![](../images/3c5ebf65-0c10-4255-b66a-12c5d049ddde-image.png)

![](../images/fe23a9e3-c93e-461a-b2b0-42d50e5ee3d5-image.png)



### âœï¸ tanh í•¨ìˆ˜(Hyperbolic tangent function)

- í•˜ì´í¼ë³¼ë¦­íƒ„ì  íŠ¸ë€ ìŒê³¡ì„  í•¨ìˆ˜ì¤‘ í•˜ë‚˜ì´ë‹¤.

  ìŒê³¡ì„  í•¨ìˆ˜ : ìŒê³¡ì„  í•¨ìˆ˜ë€ ì‚¼ê°í•¨ìˆ˜ì™€ ìœ ì‚¬í•œ ì„±ì§ˆì„ ê°€ì§€ê³ , í‘œì¤€ ìŒê³¡ì„ ì„ ë§¤ê°œë³€ìˆ˜ë¡œ í‘œì‹œí•  ë•Œ ë‚˜ì˜¤ëŠ” í•¨ìˆ˜ì´ë‹¤.

- í•˜ì´í¼ë³¼ë¦­íƒ„ì  íŠ¸ í•¨ìˆ˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ transformationí•´ì„œ ì–»ì„ ìˆ˜ ìˆë‹¤.

- íŠ¹ì§•

  - tanh í•¨ìˆ˜ëŠ” í•¨ìˆ˜ì˜ ì¤‘ì‹¬ê°’ì„ 0ìœ¼ë¡œ ì˜®ê²¨ sigmoidì˜ ìµœì í™” ê³¼ì •ì´ ëŠë ¤ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.
  - í•˜ì§€ë§Œ ë¯¸ë¶„í•¨ìˆ˜ì— ëŒ€í•´ ì¼ì •ê°’ ì´ìƒ ì»¤ì§ˆì‹œ ë¯¸ë¶„ê°’ì´ ì†Œì‹¤ë˜ëŠ” **gradient vanishing** ë¬¸ì œëŠ” ì—¬ì „íˆ ë‚¨ì•„ìˆë‹¤.

$$
tanh(x) = 2 \sigma (2x) -1 \\ tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\ tanh^{'}(x) = 1-tanh^{2}(x)
$$

![](../images/1e82ff49-7002-4f8c-a8a1-01cbc352056c-image.png)

![](../images/6ee4bc19-beba-42f3-ac26-df181ee3cdf0-image.png)

### âœï¸ ReLU í•¨ìˆ˜(Rectified Linear Unit)

- ReLuí•¨ìˆ˜ëŠ” ìµœê·¼ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜
- íŠ¹ì§•
  - x>0ì´ë©´ ê¸°ìš¸ê¸°ê°€ 1ì¸ ì§ì„ ì´ê³ , x<0ì´ë©´ í•¨ìˆ˜ê°’ì´ 0ì´ëœë‹¤.
  - sigmoid, tanh í•¨ìˆ˜ì™€ ë¹„êµì‹œ í•™ìŠµì´ í›¨ì”¬ ë¹¨ë¼ì§„ë‹¤.
  - ì—°ì‚° ë¹„ìš©ì´ í¬ì§€ì•Šê³ , êµ¬í˜„ì´ ë§¤ìš° ê°„ë‹¨í•˜ë‹¤.
  - x<0ì¸ ê°’ë“¤ì— ëŒ€í•´ì„œëŠ” ê¸°ìš¸ê¸°ê°€ 0ì´ê¸° ë•Œë¬¸ì— ë‰´ëŸ°ì´ ì£½ì„ ìˆ˜ ìˆëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.

$$
f(x) = max(0,x)
$$

![](../images/8a3333a4-f97b-4db4-8556-e49f38fbd4f2-image.png)

### âœï¸Leakly ReLU

- leakly ReLUëŠ” ReLUì˜ ë‰´ëŸ°ì´ ì£½ëŠ”(â€œDying ReLuâ€)í˜„ìƒì„ í•´ê²°í•˜ê¸°ìœ„í•´ ë‚˜ì˜¨ í•¨ìˆ˜
- Leakly ReLUëŠ” ìŒìˆ˜ì˜ xê°’ì— ëŒ€í•´ ë¯¸ë¶„ê°’ì´ 0ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ ReLUì™€ ê°™ì€ íŠ¹ì„±ì„ ê°€ì§„ë‹¤.
- ë°‘ì˜ ì‹ì—ì„œ 0.01 ëŒ€ì‹  ë‹¤ë¥¸ ë§¤ìš° ì‘ì€ ê°’ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.

$$
f(x) = max(0.01x,x)
$$



### âœï¸PReLU

- Leakly ReLUì™€ ê±°ì˜ ìœ ì‚¬í•˜ì§€ë§Œ ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° Î± ë¥¼ ì¶”ê°€í•˜ì—¬ x<0ì—ì„œ ê¸°ìš¸ê¸°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤.

$$
f(x) = max(\alpha x,x)
$$



### âœï¸Expeonential Linear Unit(ELU)

- ELUëŠ” ë¹„êµì  ê°€ì¥ ìµœê·¼ì— ë‚˜ì˜¨ í•¨ìˆ˜  [Clevert et al. ,2015](http://arxiv.org/abs/1511.07289)
- íŠ¹ì§•
  - ReLUì˜ ëª¨ë“  ì¥ì ì„ í¬í•¨í•œë‹¤.
  - â€œDying ReLUâ€ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.
  - ì¶œë ¥ê°’ì´ ê±°ì˜ zero-centeredì— ê°€ê¹ë‹¤
  - ì¼ë°˜ì ì¸ ReLUì™€ ë‹¬ë¦¬ expí•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë¹„ìš©ì´ ë°œìƒí•œë‹¤.

$$
f(x) = x \;if \; x>0 \\ f(x) = \alpha(e^{x} -1) \; if \; x<=0
$$



### âœï¸ Maxout í•¨ìˆ˜

- íŠ¹ì§•
  - ì´ í•¨ìˆ˜ëŠ” ReLUê°€ ê°€ì§€ëŠ” ëª¨ë“  ì¥ì ì„ ê°€ì¡Œë‹¤
  -  dying ReLUë¬¸ì œ ë˜í•œ í•´ê²°í•œë‹¤. 
  - í•˜ì§€ë§Œ ê³„ì‚°ëŸ‰ì´ ë³µì¡í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

$$
f(x) = max(w_{1}^{T}x+b_{1}, w_{2}^{T}x+b_{2})
$$



### âœï¸ ê·¸ë˜ì„œ ë­˜ ì‚¬ìš©í•´ì•¼í•˜ëŠ”ë°?

- ìš°ì„  ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ëŠ” ReLUì´ë‹¤. ê°„ë‹¨í•˜ê³  ì‚¬ìš©ì´ ì‰½ê¸° ë•Œë¬¸ì— ìš°ì„ ì ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©í•œë‹¤.
- ReLUë¥¼ ì‚¬ìš©í•œ ì´í›„ Leakly ReLUë“± ReLUê³„ì—´ì˜ ë‹¤ë¥¸ í•¨ìˆ˜ë„ ì‚¬ìš© í•´ë³¸ë‹¤.
- sigmoidì˜ ê²½ìš°ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ í•œë‹¤.
- tanhì˜ ê²½ìš°ë„ í° ì„±ëŠ¥ì€ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤.



------

# ğŸ“„ í™œì„±í•¨ìˆ˜(Activation Function)ëŠ” ì™œ ë¹„ì„ í˜• í•¨ìˆ˜ì—¬ì•¼ë§Œ í• ê¹Œ?

- í™œì„±í•¨ìˆ˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ì€ **ë¹„ì„ í˜•ì„±** ì´ë‹¤. 
-  ë§Œì•½ í™œì„±í•¨ìˆ˜ë¥¼ **ì„ í˜•í•¨ìˆ˜** ë¡œ ìŒ“ê²Œ ëœë‹¤ë©´ **ì¸µì„ ë¬´í•œì • ê¹Šê²Œ ìŒ“ë”ë¼ë„ [ì„ í˜•ì„±](https://ko.wikipedia.org/wiki/ì„ í˜•ì„±)** ì„ ë„ê¸° ë•Œë¬¸ì— ì„ í˜•í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ëŠ” ë‰´ëŸ´ë„·ê³¼ ê¹Šê²Œ ìŒ“ì€ ë‰´ëŸ´ë„·ì€ ì°¨ì´ê°€ ì—†ë‹¤.

------

# ğŸ“„ ì¸µì„ ì—¬ëŸ¬ê°œ ìŒ“ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?

- ì´ë¡ ì ìœ¼ë¡œëŠ” [Universal Approximation Theorem](http://neuralnetworksanddeeplearning.com/chap4.html) (**í•˜ë‚˜ì˜ ì€ë‹‰ì¸µê³¼ ë¹„ì„ í˜• í™œì„±í•¨ìˆ˜ë¥¼ ê°€ì§„ ë‰´ëŸ´ë„·ì„ ì´ìš©í•´ ì–´ë– í•œ í˜•íƒœì˜ ì—°ì†í•¨ìˆ˜ë“  ëª¨ë¸ë§ í•  ìˆ˜ ìˆë‹¤.** )ì— ì˜í•´ì„œ 2-layers Neural Net ìœ¼ë¡œë„ ì„ì˜ì˜ ì—°ì†í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤. 
- ê·¸ëŸ¬ë‚˜ ì¸µì´ ê¹Šì–´ì§„ë‹¤ë©´ ëª©ì í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ëŠ”ë° í•„ìš”í•œ ë‰´ëŸ°ì˜ ìˆ«ìê°€ í›¨ì”¬ ë¹¨ë¦¬ ì¤„ì–´ë“¤ì–´ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì¸µì„ ì—¬ëŸ¬ê°œ ìŒ“ê²Œ ëœë‹¤.( ë¬¼ë¡  ì¸µì„ ê³„ì† ê¹Šê²Œ ìŒ“ê²Œ ë˜ë©´ ìµœì í™”ì™€ ëª¨ë¸ í•™ìŠµ ë¶€ë¶„ì— ìˆì–´ ì–´ë ¤ì›Œì§„ë‹¤.)

![](../images/f2e32081-54e4-4b05-a696-592bcc9071be-image.png)

------

# ğŸ“„ ìˆœì „íŒŒ(Forwardpropagation) vs ì—­ì „íŒŒ(Backpropagation)

## âœï¸ ìˆœì „íŒŒ(Forwardpropagation)

- **ìˆœì „íŒŒ(forward propagation)** ì€ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ì˜ ì…ë ¥ì¸µë¶€í„° ì¶œë ¥ì¸µê¹Œì§€ ìˆœì„œëŒ€ë¡œ ë³€ìˆ˜ë“¤ì„ ê³„ì‚°í•˜ê³  ì €ì¥í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©° 
- í•™ìŠµê³¼ì •ì´ ì•„ë‹Œ **ë‹¨ìˆœíˆ ê°’ë“¤ì„ ë‹¤ìŒ ì¸µìœ¼ë¡œ ë„˜ê²¨ì£¼ëŠ” ê³¼ì •ì´ë‹¤.**

## âœï¸ ì—­ì „íŒŒ(Backpropagation)

- **ì—­ì „íŒŒ(back propagation)** ëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸(gradient)ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸

- ì¼ë°˜ì ìœ¼ë¡œëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ ê° ì¸µê³¼ ê´€ë ¨ëœ ëª©ì  í•¨ìˆ˜(objective function)ì˜ ì¤‘ê°„ ë³€ìˆ˜ë“¤ê³¼ íŒŒë¼ë¯¸í„°ë“¤ì˜ **ê·¸ë˜ë””ì–¸íŠ¸(gradient)ë¥¼ ì¶œë ¥ì¸µì—ì„œ ì…ë ¥ì¸µ ìˆœìœ¼ë¡œ ì—°ì‡„ë²•ì¹™(Chain rule) ì„ í†µí•´ ê³„ì‚°í•˜ê³  ì €ì¥í•œë‹¤.**

  - ì—­ì „íŒŒë¥¼ ì´ìš©í•´ ê° ì¸µì— ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° í•™ìŠµ
    $$
    \begin{bmatrix} W^{(l)}, b^{(l)} \end{bmatrix}^{L}_{l=1}
    $$

  - ì—­ìˆœìœ¼ë¡œ ì—°ì‡„ë²•ì¹™(chain-rule)ì„ í†µí•´ gradient vectorë¥¼ ì „ë‹¬
    $$
    \frac{ \partial z}{\partial x} = \frac{ \partial z}{\partial w} \frac{ \partial w}{\partial x} 
    $$

  - ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì€ chain-rule ê¸°ë°˜ì˜ ìë™ë¯¸ë¶„(auto-differentiation) ì‚¬ìš©

![](../images/764a97e0-587c-46f7-bd2f-77b291e812b5-image.png)

# ğŸ“„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ softmax í•¨ìˆ˜ê°€ ì‚¬ìš©ë˜ëŠ” ì´ìœ ê°€ ë­˜ê¹Œìš”?

- softmax í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì„ í™•ë¥ ë¡œ í•´ì„í•  ìˆ˜ ìˆê²Œ ë³€í™˜í•´ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.
- ì¦‰, softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë©´, ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ í™•ë¥ ë¡œ í‘œí˜„í•˜ì—¬ ë¶„ë¥˜ ê²°ê³¼ì™€ ë¹„êµ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
- ë”°ë¼ì„œ, ë¶„ë¥˜ ë¬¸ì œì—ì„œ softmax í•¨ìˆ˜ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
  - softmax í•¨ìˆ˜ì˜ ê²°ê³¼ì¸ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” ì‹¤ì œê°’ì˜ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì— ì‚¬ìš© í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.