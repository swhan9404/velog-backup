---
title: "확률론"
description: "딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있습니다.기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터공간을 통계적으로 해석해서 유도하게 됩니다.회귀 분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으"
date: 2021-03-10T14:36:56.532Z
tags: ["인공지능수학"]
---
# 📌확률론

# 📄 딥러닝에서 확률론이 왜 필요할까요?

- 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있습니다.
- 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터공간을 통계적으로 해석해서 유도하게 됩니다.
- 회귀 분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도합니다.
- 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도합니다.
- 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야 합니다.



# 📄 확률분포

## ✏️ 확률분포는 데이터의 초상화

- 데이터공간: 𝒳×𝒴

- 데이터공간에서 데이터를 추출하는 분포 : 𝒟

- 데이터는 확률변수: (x,y)∼𝒟

- 결합분포 P(x,y)는 𝒟를 모델링합니다.

- P(x)는 입력 x에 대한 주변확률분포로 y에 대한 정보를 주진 않습니다.

- 조건부확률분포 P(x|y)는 데이터 공간에서 입력 x와 출력 y 사이의 관계를 모델링합니다.

  

## ✏️ 이산확률변수 vs 연속확률변수

- 확률변수는 확률분포 𝒟에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분하게 됩니다.

- 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링합니다.
  $$
  \mathbb P(X \in A) = \sum_{X \in A}P(X = x)
  $$
  
- 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링합니다.

  - 밀도는 누적확률분포의 변화율을 모델링

$$
\mathbb P(X \in A) = \int_A P(x)dx
$$

## ✏️ 조건부확률과 기계학습

- 조건부확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률을 의미합니다.

  - 양성반응이라는 전제하에 유방암에 걸릴 확률 P(유방암|양성반응)

- 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용됩니다.

- 분류 문제에서 softmax(Wφ + b)은 데이터 x로부터 추출된 특징패턴 φ(x)과 가중치행렬 W을 통해 조건부확률 P(y|x)을 계산합니다.

- 회귀 문제의 경우 조건부기대값 𝔼[y|X]을 추정합니다.
  $$
  \mathbb E_{y \sim P(y \mid X)}[y \mid X] = \int_{\mathcal y} yP(y \mid X)dy)
  $$
  

- 조건부 기대값은 
  $$
  \mathbb E \Vert y - f(x) \Vert_2 를 최소화하는 함수 f(x)와 일치
  $$
  

- 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 φ을 추출합니다.



# 📄  기대값이 뭔가요?

- 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 계산할 수 있음

- 기대값(expectation)은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용

  - 이산확률분포
    $$
    \mathbb E_{X \sim P(X)}[f(X)] = \sum_{X \in \mathcal X} f(X)P(X)
    $$
    

  - 연속 확률 분포
    $$
    \mathbb E_{X \sim P(X)}[f(X)] = \int_{\mathcal X} f(X)P(X)dX
    $$
    

- 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있습니다.

  - 분산
    $$
    \mathbb V (X) = \mathbb E_{X \sim P(X)}[X - \mathbb E [X]^2]
    $$
    

  - 공분산
    $$
    Cov(X_1, X_2) = \mathbb E_{X_1,X_2 \sim P(X_1, X_2)} [(X_1 - \mathbb E [X_1])] [(X_2 - \mathbb E [X_2])]
    $$
    

# 📄 몬테카를로 샘플링

- 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분

- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용해야 한다.

  - 몬테카를로는 이산형/연속형 상관없이 성립 

    - `i.i.d.` :  독립항등분포 - 확률변수가 여러개 있을 때 이들이 상호독립적이며, 모두 동일한 확률분포를 가짐

    $$
    \mathbb E_{X \sim P(X)} [f(X)] \approx \frac{1}{N} \sum_{i=1}^N f(X^{(i)}) \\
    (i.i.d. X^{(i)} \sim P(X))
    $$

    

- 몬테카를로 샘플링은 독립추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장

### ✏️ 몬테카를로 방법을 활용한 원주율 구하기

undefined

1. 정사각형을 그린 다음, 그 안에 사분면을 삽입합니다.
2. 정사각형 위에 일정한 개수의 점을 균일하게 분포합니다.
3. 사분면 내부의 점(즉, 원점으로부터 1 미만)의 개수를 셉니다.
4. 내부의 개수와 전체 개수의 비율은 두 영역의 비율을 나타냅니다.
5. 사분면 내부의 비율에 4를 곱하여 π를 만듭니다.

여기서 두 가지의 중요한 점이 있습니다.

1. 점이 균일하게 분포되지 않으면 근사치가 떨어집니다.
2. 평균적으로 더 많은 점을 배치할수록 근사치가 개선됩니다.